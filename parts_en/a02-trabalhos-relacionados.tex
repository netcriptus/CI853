%!TEX root = ./proposta.tex

Researches involving proposes of DDoS mitigation in Cloud architectures are still considered incipient and far from a convergence. Among the few proposes for those environments, stands out the proactive framework CluB, introduced in \cite{Hazelhurst:2008:SCU:1456659.1456671}, who considers a Cloud like a network made of a set of clusters, or AS. This work suggests that certain routers will be selected and disposed in a distributed manner, for traffic analysis and consequently preventing malicious requests to reach the application. Those routers are responsible for generating authentication tokens, to legitimate the packages, being the authentication mandatory for the entry, exit and transit in the architecture. Each cluster has an authentication code, which is periodically changed, and may be generated by a hash function, like MD5 or SHA. The use of appropriate cryptographic tools and periodic infrastructure components update are part of the CluB proposal. In this framework, every and each package, malicious or not, has to be verified to go in, out or transit inside the architecture. Each allocated router must update the traffic analysis algorithm in the architecture where the CluB is in use. This issue becomes unfeasible when it comes to a Cloud, due to its architecture.


\cite{Verkaik:2006:PCD:1162666.1162673} presents another proactive proposal, which employs Communities of Interest (COI's) to capture data about the behavior of remote entities, using it to predict future behavior. Such proposal is based in the fact that clients that had legitimate relations before have a good chance of being considered legitimate again in the future. These assertions are generated from observations of normal communications of the network, and are used with server specific politics to mitigate proactively DDoS attacks, using mechanisms existent in routers.

However, identifying past clients is not so trivial. Besides the small overhead due to the verification, IP addresses are usually dynamics and the requirement of logging in for identification is not possible, given that the DDoS attack may make it become inaccessible.


In\cite{Bakshi:10}, the attacks are treated through the creation of a new instance of the application. Once a DDoS attack is detected, this proposal tries to identify the attackers through PING's: if a suspect client does not respond, it's considered an attacker. This way, only the clients that responded to the PING will be redirected to the new instance of the application. However, such approach depends on the premise that attackers will never respond to a PING and authentic clients will always respond, but that is not always true.


\cite{Walfish:2010:DDO:1731060.1731063} presents a form of attack mitigation that is classified as a resources based defense \cite{Dwork:1992:PVP:646757.705669}. Each time a certain bandwidth limit is reached due to requests to a server, this server, before its resources are exhausted, encourages its clients to send even higher volumes of traffic. Considering that the attackers are using all their capacity already, they wouldn't react to this encouragement. The intended result is that good clients overtake bad clients by capturing a higher fraction of resources from the server. The client will be served if it has enough bandwidth to outstand the attackers' traffic. Somewhat curious, this proposal causes a series of problems, like the receiving of even more traffic during an attack. A server will hardly answer to so many requests, and legitimate clients will not necessarily overtake the traffic incoming to the server.

Obviously, the effectiveness of all the schemes depend critically on the capacity of identifying or filtering legitimate clients.

 

WebSoS \cite{Stavrou:2005:WOS:1090583.1648614} is an adaptation of Secure Overlay Services (SOS) \cite{Keromytis:2002:SSO:964725.633032}, that mitigates DDoS in web servers reactively after the attack detection. With a robust traffic filtering and block of non-approved requests, a secure overlay is formed. The server uses cryptographic authentication mechanisms and a graphic Turing Test \cite{Dietrich00analyzingdistributed} in order to differentiate human clients from attack scripts. These procedures, according to the authors' tests, do not overload the service, but require that the routers in the server perimeter be configured to control the traffic, which is unfeasible to Clouds architecture.